// implement scala.Serializable, java.io.Closeable, Logging
// The entry point to Spark programming with the Dataset and DataFrame API.
// In environments this has been created in advance (eg REPL, notebooks)
import org.apache.spark.sql.SparkSession

// allows to hide some alerts
import org.apache.log4j._
Logger.getLogger("org").setLevel(Level.ERROR)

// is useful when applications want to share a SparkContext.
// So yes, you can use it to share a SparkContext object between applications.
// Yes, you can reuse broadcast variables and temporary tables in all parts.
val spark = SparkSession.builder().getOrCreate()

// k-means is one of the most widely used grouping algorithms that groups data points
// in a predefined number of clusters. The MLlib implementation includes a parallel variant
// of the k-means ++ method called kmeans ||.
// KMeans is implemented as an Estimator and generates a KMeansModel as the base model.
import org.apache.spark.ml.clustering.KMeans

// The data from the Wholesale Customers data.csv dataser is loaded in the variable "data"
val data  = spark.read.option("header","true").option("inferSchema", "true").format("csv").load("Wholesale customers data.csv")

// the columns are selected: Fresh, Milk, Groceries, Frozen, Detergents_Paper, Delicassen. and we proceed to create a set called feature_data
val feature_data = data.select($"Fresh", $"Milk", $"Grocery", $"Frozen", $"Detergents_Paper", $"Delicassen")

// This section covers algorithms for working with features, roughly divided into these groups:
// Extraction: extraction of "raw" data characteristics
// Transformation: scale, convert or modify features
// Selection: select a subset of a larger feature set
// Locality Sensitive Hashing (LSH): This class of algorithms combines aspects of feature transformation with other algorithms.
import org.apache.spark.ml.feature.VectorAssembler
// Factory methods for working with vectors. Note that dense vectors simply
// are rendered as NumPy array objects, so there is no need to convert them to use them in
// MLlib. For sparse vectors, the factory methods in this class create a type compatible with MLlib,
// or users can pass the column vectors scipy.sparse from SciPy.
import org.apache.spark.ml.linalg.Vectors

// a new Vector Assembler object is created for the columns: Fresh, Milk, Grocery, Frozen, Detergents_Paper, Delicassen. as an input set.
val assembler = new VectorAssembler().setInputCols(Array("Fresh", "Milk", "Grocery", "Frozen", "Detergents_Paper", "Delicassen")).setOutputCol("features")

// VectorAssembler is a transformer that combines a given list of columns into a single vector column.
// It is useful to combine raw features and features generated by different feature transformers in one
// vector of individual characteristics, to train ML models such as logistic regression and decision trees. VectorAssembler
// accepts the following types of input columns: all numeric types, boolean type and vector type. In every row
// the values ​​of the input columns will be concatenated into a vector in the specified order.
val training_data = assembler.transform(feature_data).select($"features")

// k-means is one of the most widely used grouping algorithms that groups data points into
// a predefined number of clusters. The MLlib implementation includes a parallel variant of the k-means ++ method called kmeans ||.
// KMeans is implemented as an Estimator and generates a KMeansModel as the base model.
val kmeans = new KMeans().setK(3).setSeed(1L)
val model = kmeans.fit(training_data)

// MLlib supports grouping of k-means, one of the most widely used grouping algorithms that groups data points into predefined values
// number of groups. The MLlib implementation includes a parallel variant of the k-means ++ method called kmeans ||.
val WSSEw = model.computeCost(training_data)
println(s"Within set sum of Squared Errors = $WSSEw")

// Show the result.
println("Cluster Centers: ")
model.clusterCenters.foreach(println)